{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport copy\nimport os\nimport torch\nfrom PIL import Image\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\nfrom torchvision import utils\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-23T05:13:39.354372Z","iopub.execute_input":"2024-07-23T05:13:39.355264Z","iopub.status.idle":"2024-07-23T05:13:45.586264Z","shell.execute_reply.started":"2024-07-23T05:13:39.355226Z","shell.execute_reply":"2024-07-23T05:13:45.585341Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary\nimport torchsummary ","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:13:45.588103Z","iopub.execute_input":"2024-07-23T05:13:45.588862Z","iopub.status.idle":"2024-07-23T05:14:01.607571Z","shell.execute_reply.started":"2024-07-23T05:13:45.588818Z","shell.execute_reply":"2024-07-23T05:14:01.606242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Prep**","metadata":{}},{"cell_type":"code","source":"labels_df = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/train_labels.csv')\nlabels_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:01.609171Z","iopub.execute_input":"2024-07-23T05:14:01.609516Z","iopub.status.idle":"2024-07-23T05:14:01.999207Z","shell.execute_reply.started":"2024-07-23T05:14:01.609484Z","shell.execute_reply":"2024-07-23T05:14:01.997970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:02.002110Z","iopub.execute_input":"2024-07-23T05:14:02.002499Z","iopub.status.idle":"2024-07-23T05:14:02.009623Z","shell.execute_reply.started":"2024-07-23T05:14:02.002450Z","shell.execute_reply":"2024-07-23T05:14:02.008453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_df['label'].value_counts() #not balanced either stratify or take an equal portion of each\n    # 0 - non_malignant cells\n    # 1 - malignant cells\n                        ","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:02.011202Z","iopub.execute_input":"2024-07-23T05:14:02.012040Z","iopub.status.idle":"2024-07-23T05:14:02.077792Z","shell.execute_reply.started":"2024-07-23T05:14:02.011968Z","shell.execute_reply":"2024-07-23T05:14:02.076587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0) # fix random seed\n\nclass pytorch_data(Dataset):\n    \n    def __init__(self,data_dir,transform,data_type=\"train\"):      \n    \n        # Get Image File Names\n        cdm_data=os.path.join(data_dir,data_type)  # directory of files\n        \n        file_names = os.listdir(cdm_data) # get list of images in that directory  \n        idx_choose = np.random.choice(np.arange(len(file_names)), \n                                      10000, #taking 10k of all the data\n                                      replace=False).tolist()\n        file_names_sample = [file_names[x] for x in idx_choose]\n        self.full_filenames = [os.path.join(cdm_data, f) for f in file_names_sample]   # get the full path to images\n        \n        # Get Labels\n        labels_data=os.path.join(data_dir,\"train_labels.csv\") \n        labels_df=pd.read_csv(labels_data)\n        labels_df.set_index(\"id\", inplace=True) # set data frame index to id\n        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in file_names_sample]  # obtained labels from df\n        self.transform = transform\n      \n    def __len__(self):\n        return len(self.full_filenames) # size of dataset\n      \n    def __getitem__(self, idx):\n        # open image, apply transforms and return with label\n        image = Image.open(self.full_filenames[idx])  # Open Image with PIL\n        image = self.transform(image) # Apply Specific Transformation to Image\n        return image, self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:02.081014Z","iopub.execute_input":"2024-07-23T05:14:02.081350Z","iopub.status.idle":"2024-07-23T05:14:02.096088Z","shell.execute_reply.started":"2024-07-23T05:14:02.081321Z","shell.execute_reply":"2024-07-23T05:14:02.094886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as transforms\ndata_transformer = transforms.Compose([transforms.ToTensor(),\n                                       transforms.Resize((46,46))]) ","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:02.097859Z","iopub.execute_input":"2024-07-23T05:14:02.098687Z","iopub.status.idle":"2024-07-23T05:14:02.109370Z","shell.execute_reply.started":"2024-07-23T05:14:02.098647Z","shell.execute_reply":"2024-07-23T05:14:02.108319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define an object of the custom dataset for the train folder.\ndata_dir = '/kaggle/input/histopathologic-cancer-detection/'\nimg_dataset = pytorch_data(data_dir, data_transformer, \"train\") # Histopathalogic images\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:02.111048Z","iopub.execute_input":"2024-07-23T05:14:02.111777Z","iopub.status.idle":"2024-07-23T05:14:05.049276Z","shell.execute_reply.started":"2024-07-23T05:14:02.111743Z","shell.execute_reply":"2024-07-23T05:14:05.048254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Prep**","metadata":{}},{"cell_type":"code","source":"len_img=len(img_dataset)\nlen_train=int(0.8*len_img)\nlen_val=len_img-len_train\n\n# Split Pytorch tensor\ntrain_ts,val_ts=random_split(img_dataset,\n                             [len_train,len_val]) # random split 80/20\n\nprint(\"train dataset size:\", len(train_ts))\nprint(\"validation dataset size:\", len(val_ts))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:05.050686Z","iopub.execute_input":"2024-07-23T05:14:05.051067Z","iopub.status.idle":"2024-07-23T05:14:05.084204Z","shell.execute_reply.started":"2024-07-23T05:14:05.051031Z","shell.execute_reply":"2024-07-23T05:14:05.083147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Augmentation","metadata":{}},{"cell_type":"code","source":"#train data aug\ntr_transf = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5), \n    transforms.RandomVerticalFlip(p=0.5),  #probability of 0.5\n    transforms.RandomRotation(45),         # -45,45 degree rotation\n    transforms.ToTensor()])","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:39.339023Z","iopub.execute_input":"2024-07-23T05:14:39.339972Z","iopub.status.idle":"2024-07-23T05:14:39.345818Z","shell.execute_reply.started":"2024-07-23T05:14:39.339932Z","shell.execute_reply":"2024-07-23T05:14:39.344634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_transf = transforms.Compose([\n    transforms.ToTensor()])\n\n# After defining the transformations, overwrite the transform functions of train_ts, val_ts\ntrain_ts.transform=tr_transf\nval_ts.transform=val_transf","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:39.830767Z","iopub.execute_input":"2024-07-23T05:14:39.831412Z","iopub.status.idle":"2024-07-23T05:14:39.836266Z","shell.execute_reply.started":"2024-07-23T05:14:39.831376Z","shell.execute_reply":"2024-07-23T05:14:39.835197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dl = DataLoader(train_ts , batch_size = 32 , shuffle = True)\nval_dl = DataLoader(val_ts , batch_size = 32 , shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:40.222223Z","iopub.execute_input":"2024-07-23T05:14:40.222970Z","iopub.status.idle":"2024-07-23T05:14:40.228322Z","shell.execute_reply.started":"2024-07-23T05:14:40.222935Z","shell.execute_reply":"2024-07-23T05:14:40.227304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x,y in train_dl:\n    print(x.shape,y)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:40.433195Z","iopub.execute_input":"2024-07-23T05:14:40.433988Z","iopub.status.idle":"2024-07-23T05:14:40.818269Z","shell.execute_reply.started":"2024-07-23T05:14:40.433953Z","shell.execute_reply":"2024-07-23T05:14:40.817217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Binary Classifier","metadata":{}},{"cell_type":"code","source":"def findConv2dOutShape(hin,win,conv,pool=2):\n    # get conv arguments\n    kernel_size=conv.kernel_size\n    stride=conv.stride\n    padding=conv.padding\n    dilation=conv.dilation\n\n    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n\n    if pool:\n        hout/=pool\n        wout/=pool\n    return int(hout),int(wout)\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Neural Network\nclass Network(nn.Module):\n    \n    # Network Initialisation\n    def __init__(self, params):\n        \n        super(Network, self).__init__()\n    \n        Cin,Hin,Win=params[\"shape_in\"]\n        init_f=params[\"initial_filters\"] \n        num_fc1=params[\"num_fc1\"]  \n        num_classes=params[\"num_classes\"] \n        self.dropout_rate=params[\"dropout_rate\"] \n        \n        # Convolution Layers\n        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n        h,w=findConv2dOutShape(h,w,self.conv2)\n        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n        h,w=findConv2dOutShape(h,w,self.conv3)\n        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n        h,w=findConv2dOutShape(h,w,self.conv4)\n        \n        # compute the flatten size\n        self.num_flatten=h*w*8*init_f\n        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n        self.fc2 = nn.Linear(num_fc1, num_classes)\n\n    def forward(self,X):\n        \n        # Convolution & Pool Layers\n        X = F.relu(self.conv1(X)); \n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv4(X))\n        X = F.max_pool2d(X, 2, 2)\n\n        X = X.view(-1, self.num_flatten)\n        \n        X = F.relu(self.fc1(X))\n        X=F.dropout(X, self.dropout_rate)\n        X = self.fc2(X)\n        return F.log_softmax(X, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:40.821121Z","iopub.execute_input":"2024-07-23T05:14:40.821477Z","iopub.status.idle":"2024-07-23T05:14:40.839052Z","shell.execute_reply.started":"2024-07-23T05:14:40.821447Z","shell.execute_reply":"2024-07-23T05:14:40.837938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_model={\n        \"shape_in\": (3,46,46), \n        \"initial_filters\": 8,    \n        \"num_fc1\": 100,\n        \"dropout_rate\": 0.25,\n        \"num_classes\": 2}\n\n# Create instantiation of Network class\ncnn_model = Network(params_model)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = cnn_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:40.985905Z","iopub.execute_input":"2024-07-23T05:14:40.986285Z","iopub.status.idle":"2024-07-23T05:14:41.176258Z","shell.execute_reply.started":"2024-07-23T05:14:40.986256Z","shell.execute_reply":"2024-07-23T05:14:41.175212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary \nsummary(cnn_model, input_size=(3, 46, 46),device=device.type)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:41.189725Z","iopub.execute_input":"2024-07-23T05:14:41.190086Z","iopub.status.idle":"2024-07-23T05:14:41.836612Z","shell.execute_reply.started":"2024-07-23T05:14:41.190057Z","shell.execute_reply":"2024-07-23T05:14:41.835594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import optim\n\nloss_func = nn.NLLLoss(reduction=\"sum\") #log_softmax loss function \nopt = optim.Adam(cnn_model.parameters(), lr=3e-4) # adam optimizer \n\n\nlr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:41.838718Z","iopub.execute_input":"2024-07-23T05:14:41.839422Z","iopub.status.idle":"2024-07-23T05:14:41.845598Z","shell.execute_reply.started":"2024-07-23T05:14:41.839384Z","shell.execute_reply":"2024-07-23T05:14:41.844385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training the Model**","metadata":{}},{"cell_type":"code","source":"# Function to get the learning rate\ndef get_lr(opt):\n    for param_group in opt.param_groups:\n        return param_group['lr']\n\n# Function to compute the loss value per batch of data\ndef loss_batch(loss_func, output, target, opt=None):\n    \n    loss = loss_func(output, target) # get loss\n    pred = output.argmax(dim=1, keepdim=True) # Get Output Class\n    metric_b=pred.eq(target.view_as(pred)).sum().item() # get performance metric\n    \n    if opt is not None:\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n    return loss.item(), metric_b\n\n# Compute the loss value & performance metric for the entire dataset (epoch)\ndef loss_epoch(model,loss_func,dataset_dl,opt=None):\n    \n    run_loss=0.0 \n    t_metric=0.0\n    len_data=len(dataset_dl.dataset)\n\n    # internal loop over dataset\n    for xb, yb in dataset_dl:\n        # move batch to device\n        xb=xb.to(device)\n        yb=yb.to(device)\n        output=model(xb) # get model output\n        loss_b,metric_b=loss_batch(loss_func, output, yb, opt) # get loss per batch\n        run_loss+=loss_b        # update running loss\n\n        if metric_b is not None: # update running metric\n            t_metric+=metric_b    \n    \n    loss=run_loss/float(len_data)  # average loss value\n    metric=t_metric/float(len_data) # average metric value\n    \n    return loss, metric","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:41.846899Z","iopub.execute_input":"2024-07-23T05:14:41.847512Z","iopub.status.idle":"2024-07-23T05:14:41.861821Z","shell.execute_reply.started":"2024-07-23T05:14:41.847481Z","shell.execute_reply":"2024-07-23T05:14:41.860879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_train={\n \"train\": train_dl,\"val\": val_dl,\n \"epochs\": 50,\n \"optimiser\": optim.Adam(cnn_model.parameters(),\n                         lr=3e-4),\n \"lr_change\": ReduceLROnPlateau(opt,\n                                mode='min',\n                                factor=0.5,\n                                patience=20,\n                                verbose=0),\n \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n \"weight_path\": \"weights.pt\",\n \"check\": False, \n}","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:42.351287Z","iopub.execute_input":"2024-07-23T05:14:42.351953Z","iopub.status.idle":"2024-07-23T05:14:42.358760Z","shell.execute_reply.started":"2024-07-23T05:14:42.351920Z","shell.execute_reply":"2024-07-23T05:14:42.357608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import trange, tqdm\n\ndef train_val(model, params,verbose=False):\n    \n    epochs=params[\"epochs\"]\n    loss_func=params[\"f_loss\"]\n    opt=params[\"optimiser\"]\n    train_dl=params[\"train\"]\n    val_dl=params[\"val\"]\n    lr_scheduler=params[\"lr_change\"]\n    weight_path=params[\"weight_path\"]\n    \n    loss_history={\"train\": [],\"val\": []} # history of loss values in each epoch\n    metric_history={\"train\": [],\"val\": []} # histroy of metric values in each epoch\n    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n    best_loss=float('inf') # initialize best loss to a large value\n \n\n    for epoch in tqdm(range(epochs)):\n        \n        #get the learning rate \n        current_lr=get_lr(opt)\n        if(verbose):\n            print('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\n      \n        \n        model.train()\n        train_loss, train_metric = loss_epoch(model,loss_func,train_dl,opt)\n\n        # collect losses\n        loss_history[\"train\"].append(train_loss)\n        metric_history[\"train\"].append(train_metric)\n        \n     \n    \n    \n        #eval\n        model.eval()\n        with torch.no_grad():\n            val_loss, val_metric = loss_epoch(model,loss_func,val_dl)\n        \n        # store best model\n        if(val_loss < best_loss):\n            best_loss = val_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            \n            # store weights into a local file\n            torch.save(model.state_dict(), weight_path)\n            if(verbose):\n                print(\"Copied best model weights!\")\n        \n        # collect loss and metric for validation dataset\n        loss_history[\"val\"].append(val_loss)\n        metric_history[\"val\"].append(val_metric)\n        \n        # learning rate schedule\n        lr_scheduler.step(val_loss)\n        if current_lr != get_lr(opt):\n            if(verbose):\n                print(\"Loading best model weights!\")\n            model.load_state_dict(best_model_wts) \n\n        if(verbose):\n            print(f\"train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\")\n            print(\"-\"*10) \n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n        \n    return model, loss_history, metric_history","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:14:42.831381Z","iopub.execute_input":"2024-07-23T05:14:42.832373Z","iopub.status.idle":"2024-07-23T05:14:42.846497Z","shell.execute_reply.started":"2024-07-23T05:14:42.832312Z","shell.execute_reply":"2024-07-23T05:14:42.845192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_train={\n \"train\": train_dl,\"val\": val_dl,\n \"epochs\": 50,\n \"optimiser\": optim.Adam(cnn_model.parameters(),lr=3e-4),\n \"lr_change\": ReduceLROnPlateau(opt,\n                                mode='min',\n                                factor=0.5,\n                                patience=20,\n                                verbose=0),\n \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n \"weight_path\": \"weights.pt\",\n}\n\n# train and validate the model\n\ncnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:15:16.301220Z","iopub.execute_input":"2024-07-23T05:15:16.301647Z","iopub.status.idle":"2024-07-23T05:31:56.115163Z","shell.execute_reply.started":"2024-07-23T05:15:16.301612Z","shell.execute_reply":"2024-07-23T05:31:56.114118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss and eval metric visualization \n\nimport seaborn as sns; sns.set(style='whitegrid')\n\nepochs=params_train[\"epochs\"]\n\nfig,ax = plt.subplots(1,2,figsize=(12,5))\n\nsns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"train\"],ax=ax[0],label='loss_hist[\"train\"]')\nsns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"val\"],ax=ax[0],label='loss_hist[\"val\"]')\nsns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"train\"],ax=ax[1],label='metric_hist[\"train\"]')\nsns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"val\"],ax=ax[1],label='metric_hist[\"val\"]')\nplt.title('Convergence History')\n\n#overfit","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:32:02.485033Z","iopub.execute_input":"2024-07-23T05:32:02.485476Z","iopub.status.idle":"2024-07-23T05:32:04.251707Z","shell.execute_reply.started":"2024-07-23T05:32:02.485442Z","shell.execute_reply":"2024-07-23T05:32:04.250502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Testing the Performance of the Model**","metadata":{}},{"cell_type":"code","source":"class pytorchdata_test(Dataset):\n    \n    def __init__(self, data_dir, transform,data_type=\"train\"):\n        \n        path2data = os.path.join(data_dir,data_type)\n        filenames = os.listdir(path2data)\n        self.full_filenames = [os.path.join(path2data, f) for f in filenames]\n        \n        # labels are in a csv file named train_labels.csv\n        csv_filename=\"sample_submission.csv\"\n        path2csvLabels=os.path.join(data_dir,csv_filename)\n        labels_df=pd.read_csv(path2csvLabels)\n        \n        # set data frame index to id\n        labels_df.set_index(\"id\", inplace=True)\n        \n        # obtain labels from data frame\n        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in filenames]\n        self.transform = transform       \n        \n    def __len__(self):\n        # return size of dataset\n        return len(self.full_filenames)\n    \n    def __getitem__(self, idx):\n        # open image, apply transforms and return with label\n        image = Image.open(self.full_filenames[idx]) # PIL image\n        image = self.transform(image)\n        return image, self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:34:21.333665Z","iopub.execute_input":"2024-07-23T05:34:21.334018Z","iopub.status.idle":"2024-07-23T05:34:21.344647Z","shell.execute_reply.started":"2024-07-23T05:34:21.333991Z","shell.execute_reply":"2024-07-23T05:34:21.343380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model.load_state_dict(torch.load('weights.pt'))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:36:18.476019Z","iopub.execute_input":"2024-07-23T05:36:18.476817Z","iopub.status.idle":"2024-07-23T05:36:18.491995Z","shell.execute_reply.started":"2024-07-23T05:36:18.476775Z","shell.execute_reply":"2024-07-23T05:36:18.490457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_sub = \"/kaggle/input/histopathologic-cancer-detection/sample_submission.csv\"\nlabels_df = pd.read_csv(path_sub)\nlabels_df.head()\nlabels_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:36:32.675714Z","iopub.execute_input":"2024-07-23T05:36:32.676150Z","iopub.status.idle":"2024-07-23T05:36:32.791447Z","shell.execute_reply.started":"2024-07-23T05:36:32.676117Z","shell.execute_reply":"2024-07-23T05:36:32.790342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/histopathologic-cancer-detection/'\n\ndata_transformer = transforms.Compose([transforms.ToTensor(),\n                                       transforms.Resize((46,46))])\n\nimg_dataset_test = pytorchdata_test(data_dir,data_transformer,data_type=\"test\")\nprint(len(img_dataset_test), 'samples found')","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:36:43.598586Z","iopub.execute_input":"2024-07-23T05:36:43.599762Z","iopub.status.idle":"2024-07-23T05:36:46.968129Z","shell.execute_reply.started":"2024-07-23T05:36:43.599709Z","shell.execute_reply":"2024-07-23T05:36:46.966963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model,dataset,device,num_classes=2):\n    \n    len_data=len(dataset)\n    y_out=torch.zeros(len_data,num_classes) # initialize output tensor on CPU\n    y_gt=np.zeros((len_data),dtype=\"uint8\") # initialize ground truth on CPU\n    model=model.to(device) # move model to device\n    \n    with torch.no_grad():\n        for i in tqdm(range(len_data)):\n            x,y=dataset[i]\n            y_gt[i]=y\n            y_out[i]=model(x.unsqueeze(0).to(device))\n\n    return y_out.numpy(),y_gt  ","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:37:12.782919Z","iopub.execute_input":"2024-07-23T05:37:12.783341Z","iopub.status.idle":"2024-07-23T05:37:12.791755Z","shell.execute_reply.started":"2024-07-23T05:37:12.783307Z","shell.execute_reply":"2024-07-23T05:37:12.790497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_out,_ = inference(cnn_model,img_dataset_test, device)     ","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:37:25.161801Z","iopub.execute_input":"2024-07-23T05:37:25.162189Z","iopub.status.idle":"2024-07-23T05:47:15.477918Z","shell.execute_reply.started":"2024-07-23T05:37:25.162158Z","shell.execute_reply":"2024-07-23T05:47:15.476835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class predictions 0,1\ny_test_pred=np.argmax(y_test_out,axis=1)\nprint(y_test_pred.shape)\nprint(y_test_pred[0:5])","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:47:18.178546Z","iopub.execute_input":"2024-07-23T05:47:18.178975Z","iopub.status.idle":"2024-07-23T05:47:18.186722Z","shell.execute_reply.started":"2024-07-23T05:47:18.178943Z","shell.execute_reply":"2024-07-23T05:47:18.185426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# probabilities of predicted selection\npreds = np.exp(y_test_out[:, 1])\nprint(preds.shape)\nprint(preds[0:5])","metadata":{"execution":{"iopub.status.busy":"2024-07-23T05:47:32.169844Z","iopub.execute_input":"2024-07-23T05:47:32.170825Z","iopub.status.idle":"2024-07-23T05:47:32.178051Z","shell.execute_reply.started":"2024-07-23T05:47:32.170789Z","shell.execute_reply":"2024-07-23T05:47:32.176589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model can be improved by : \n\n1. Using the whole dataset for train and validation\n2. Hyper-param tuning\n3. adding more epochs for training since our model was fitting well at the end of the training based on the plot ","metadata":{}}]}